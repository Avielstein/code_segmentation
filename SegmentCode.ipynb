{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all predictions from one window\n",
    "def get_window_predictions(window, model):\n",
    "    preds, h = model(torch.tensor([window]))\n",
    "    preds = torch.flatten(torch.sigmoid(preds))\n",
    "    preds = preds.detach().numpy()\n",
    "    return preds\n",
    "\n",
    "#get_window_predictions(test_x[0],LSTM_LM_net_trained)\n",
    "\n",
    "\n",
    "\n",
    "#NOTE, THIS ONLY GETS THE PREDICTED BREAK POINTS FROM A PREDICTION\n",
    "#WITH THE NEWLINE TOKEN AT CENTER OF WINDOW\n",
    "def get_predicted_break_points(code_windows, model):\n",
    "    start = 0\n",
    "    code  =[]\n",
    "    break_points = []\n",
    "    mid_points_preds = []\n",
    "    print(len(code_windows))\n",
    "    for window_i in range(len(code_windows)):\n",
    "        #get window, which has our tokens\n",
    "        window = code_windows[window_i]\n",
    "        window_predictions = get_window_predictions(window,model)\n",
    "        #mid = math.ceil(len(window)/2)\n",
    "        mid = int(len(window)/2) #actually we need to round down...\n",
    "        mid_token = tokenizer.decode(int(window[mid]))\n",
    "        mid_pred = window_predictions[mid]\n",
    "        mid_points_preds.append(mid_pred)\n",
    "        \n",
    "        #if only new lines\n",
    "        #if mid_token[-7:]=='NEWLINE':\n",
    "        #    if round(mid_pred) == 1:\n",
    "        #        break_points.append(window_i)\n",
    "        #print(mid_token,mid_pred)\n",
    "        if round(mid_pred) == 1:\n",
    "            print(mid_token, mid_pred)\n",
    "            break_points.append(window_i)\n",
    "                \n",
    "                \n",
    "        code.append(mid_token)\n",
    "        start+=1\n",
    "    \n",
    "    #print(max(mid_points_preds))\n",
    "    return code, break_points\n",
    "    \n",
    "#code_windows = segments['0']['x']\n",
    "#code, breaks = get_predicted_break_points(code_windows,LSTM_LM_net_trained)\n",
    "#print(breaks)\n",
    "\n",
    "#from code segmentation file\n",
    "def insert_comments(code, break_spots, comment='\\n'+'*'*8+'\\n',at_begining=True):\n",
    "    #if there is a a comment at begining of snippet\n",
    "    if at_begining:\n",
    "        #adds a notation to add a 0\n",
    "        #at beigning of break spots too\n",
    "        break_spots.insert(0,0)\n",
    "    \n",
    "    #go through breaks backwards\n",
    "    #so as not to mess up break \n",
    "    #spots as we would if we went forward\n",
    "    for b in break_spots[::-1]:\n",
    "        code.insert(b,comment)\n",
    "    return code\n",
    "\n",
    "#get all python files and associated task\n",
    "def get_GCJ_code(sample_number = 0):\n",
    "    code = ''\n",
    "    sample=sample_number\n",
    "    count=0\n",
    "    for i in range(len(df['flines'])):\n",
    "        file_name = df['file'][i]\n",
    "        file_type = file_name.split('.')[-1]\n",
    "        if file_type == 'py':\n",
    "            count+=1\n",
    "            if sample==count:\n",
    "                code+=df['flines'][i]\n",
    "                break\n",
    "    return code\n",
    "\n",
    "def centered_sliding_window(token_list, window_diamiter,encode=False,PAD='unk'):\n",
    "    windows = []\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        #print(token_list)\n",
    "        #input()\n",
    "        \n",
    "        window = []\n",
    "        \n",
    "        #if we have to pad the begining\n",
    "        if i < window_diamiter:\n",
    "            before_len = window_diamiter-i\n",
    "            before = [PAD]*before_len+token_list[0:i]\n",
    "        else:\n",
    "            before = token_list[i-window_diamiter:i]\n",
    "        \n",
    "        #if we have to pad the end\n",
    "        if i+window_diamiter>=len(token_list):\n",
    "            after_len = (i+1+window_diamiter)-len(token_list)\n",
    "            after = token_list[i+1:i+1+window_diamiter]+[PAD]*after_len\n",
    "\n",
    "        else:\n",
    "            after = token_list[i+1:i+1+window_diamiter]\n",
    "        \n",
    "        #put it togeather\n",
    "        #print('------')\n",
    "        #print('before:',before)\n",
    "        #print('center:',token_list[i])\n",
    "        #print('after:',after)\n",
    "        window = before + [token_list[i]] + after\n",
    "        #for encoding code if we want\n",
    "        if encode:\n",
    "            new_window = []\n",
    "            #print(window)\n",
    "            #input()\n",
    "            for i in window:\n",
    "                encoded = tokenizer.encode(i)\n",
    "                if len(encoded)>1:       \n",
    "                    x=encoded[1]\n",
    "                    if type(x)==list:\n",
    "                        new_window.append(x[0])\n",
    "                    else:\n",
    "                        new_window.append(x)\n",
    "                elif len(encoded)==1:\n",
    "                    if type(encoded)==list:\n",
    "                        new_window.append(encoded[0])\n",
    "                    else:\n",
    "                        new_window.append(encoded)\n",
    "                else:\n",
    "                    #for some reason it finds the unicode stuff __\n",
    "                    pass\n",
    "                    #print(window)\n",
    "                    #print(i)\n",
    "                    #print(encoded)\n",
    "                    #input()\n",
    "            #print(window)\n",
    "            #print(len(window))\n",
    "            #print(len(tokenizer.decode(window)))\n",
    "            #print(tokenizer.decode(window))\n",
    "            #print(len(tokenizer.encode(window)))\n",
    "            #window = tokenizer.encode(tokenizer.decode(window))\n",
    "            window = new_window\n",
    "        #print(window)\n",
    "        #print(len(window))\n",
    "        #input()\n",
    "\n",
    "        #save windowz\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "\n",
    "###\n",
    "# adapted from the PyTorch examples. for the full PyTorch LM example, see: \n",
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "###\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    \"\"\"Model feeds pre-trained embeddings through a series of biLSTM\n",
    "       layers, followed by a linear vocabulary decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, lstm_layers, word_vectors, \n",
    "                 dropout=0.05, bidirectional = True):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = word_vectors.shape[0]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        # blank embed layer starting from GloVe pre-trained vectors\n",
    "        self._embed = nn.Embedding.from_pretrained(word_vectors, freeze=False)        \n",
    "        self._drop = nn.Dropout(dropout)\n",
    "\n",
    "        self._lstm = nn.LSTM(in_dim, hidden_dim, num_layers = lstm_layers, dropout = dropout,\n",
    "                             bidirectional = bidirectional, batch_first=True)\n",
    "        self._ReLU = nn.ReLU()\n",
    "        self._pred = nn.Linear((2 if bidirectional else 1)*hidden_dim, \n",
    "                               #self.vocab_size)\n",
    "                               1) #only 1 or zeros here \n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self._drop(self._embed(x))\n",
    "        z, h = self._lstm(e)\n",
    "        z_drop = self._drop(z)\n",
    "        s = self._pred(self._ReLU(z_drop))\n",
    "        #s = s.view(-1, self.vocab_size)\n",
    "        s = s.squeeze()\n",
    "        return s, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.lstm_layers, batch_size, self.hidden_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "## use saved model\n",
    "#######\n",
    "\n",
    "torch.manual_seed(691)\n",
    "\n",
    "#vocab size from sentence peice\n",
    "vocab_size = 10000 #same as sentence peice\n",
    "vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "# randomly initialize our word vectors!\n",
    "vocab_dim = 256\n",
    "word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "word_vectors.shape, word_vectors\n",
    "\n",
    "#set up model\n",
    "hidden_dim = 200\n",
    "lstm_layers = 2\n",
    "LSTM_LM_net_trained = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "#[TODO]: fix so it works\n",
    "#https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "#https://stackoverflow.com/questions/61242966/pytorch-attributeerror-function-object-has-no-attribute-copy\n",
    "\n",
    "name = 'long_py'\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                        #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                        input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "#load weights into model\n",
    "LSTM_LM_net_trained.load_state_dict(torch.load('./data/'+name+'biLSTM_LM.pt'))\n",
    "LSTM_LM_net_trained.eval()\n",
    "\n",
    "#instantiate tokenizer model\n",
    "tokenizer = Tokenizer(name+'_tokenizer.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples=1\\ni = np.random.choice(range(0,segment_length),num_samples)\\ncode_windows = segments[str(i[0])]['x']\\n\\ncode, breaks = get_predicted_break_points(code_windows,LSTM_LM_net_trained)\\n\\nprint(breaks)\\n\\n#from code segmentation file\\ncomments_added = insert_comments(code,breaks)\\ncomments_added_decoded = tokenizer.decode(comments_added)\\ncomments_added_token_string = ''.join(comments_added_decoded)\\ncomments_added_token_string = comments_added_token_string.replace('SPACE',' ')\\ncomments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\\ncomments_added_token_string = comments_added_token_string.replace('TAB','\\t')\\nprint(comments_added_token_string)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples=1\n",
    "i = np.random.choice(range(0,segment_length),num_samples)\n",
    "code_windows = segments[str(i[0])]['x']\n",
    "\n",
    "code, breaks = get_predicted_break_points(code_windows,LSTM_LM_net_trained)\n",
    "\n",
    "print(breaks)\n",
    "\n",
    "#from code segmentation file\n",
    "comments_added = insert_comments(code,breaks)\n",
    "comments_added_decoded = tokenizer.decode(comments_added)\n",
    "comments_added_token_string = ''.join(comments_added_decoded)\n",
    "comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "print(comments_added_token_string)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'year', 'round', 'username', 'task', 'solution', 'file', 'full_path', 'flines']\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/54145433/reading-csv-file-line-by-line-and-save-lines-which-are-satisfying-certain-condit\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"gcj2008.csv\")\n",
    "\n",
    "headers = []\n",
    "\n",
    "for header in df:\n",
    "    headers.append(header)\n",
    "    \n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "574\n",
      "-----------------------------------------------------\n",
      "553\n",
      "-----------------------------------------------------\n",
      "915\n",
      "-----------------------------------------------------\n",
      "2469\n",
      "-----------------------------------------------------\n",
      "672\n",
      "-----------------------------------------------------\n",
      "1393\n",
      "-----------------------------------------------------\n",
      "575\n",
      "-----------------------------------------------------\n",
      "1418\n",
      "-----------------------------------------------------\n",
      "1365\n",
      "-----------------------------------------------------\n",
      "976\n",
      "-----------------------------------------------------\n",
      "589\n",
      "-----------------------------------------------------\n",
      "426\n",
      "-----------------------------------------------------\n",
      "670\n",
      "-----------------------------------------------------\n",
      "1379\n",
      "-----------------------------------------------------\n",
      "857\n",
      "NEWLINE 0.6786981\n",
      "15 1\n",
      "-----------------------------------------------------\n",
      "1852\n",
      "-----------------------------------------------------\n",
      "1329\n",
      "NEWLINE 0.54648113\n",
      "17 1\n",
      "-----------------------------------------------------\n",
      "794\n",
      "-----------------------------------------------------\n",
      "4960\n",
      "NEWLINE 0.5160842\n",
      "19 1\n",
      "-----------------------------------------------------\n",
      "5757\n",
      "NEWLINE 0.5738884\n",
      "NEWLINE 0.5160842\n",
      "20 2\n",
      "-----------------------------------------------------\n",
      "5159\n",
      "NEWLINE 0.51569194\n",
      "NEWLINE 0.5160842\n",
      "21 2\n",
      "-----------------------------------------------------\n",
      "631\n",
      "-----------------------------------------------------\n",
      "2073\n",
      "-----------------------------------------------------\n",
      "1317\n",
      "-----------------------------------------------------\n",
      "1370\n",
      "-----------------------------------------------------\n",
      "727\n",
      "-----------------------------------------------------\n",
      "409\n",
      "-----------------------------------------------------\n",
      "421\n",
      "-----------------------------------------------------\n",
      "536\n",
      "-----------------------------------------------------\n",
      "2555\n",
      "-----------------------------------------------------\n",
      "473\n",
      "-----------------------------------------------------\n",
      "1118\n",
      "-----------------------------------------------------\n",
      "530\n",
      "-----------------------------------------------------\n",
      "1205\n",
      "-----------------------------------------------------\n",
      "2665\n",
      "-----------------------------------------------------\n",
      "2665\n",
      "-----------------------------------------------------\n",
      "1852\n",
      "-----------------------------------------------------\n",
      "1852\n",
      "-----------------------------------------------------\n",
      "1018\n",
      "-----------------------------------------------------\n",
      "1774\n",
      "-----------------------------------------------------\n",
      "1775\n",
      "-----------------------------------------------------\n",
      "946\n",
      "-----------------------------------------------------\n",
      "2004\n",
      "NEWLINE 0.6220018\n",
      "43 1\n",
      "-----------------------------------------------------\n",
      "917\n",
      "-----------------------------------------------------\n",
      "475\n",
      "-----------------------------------------------------\n",
      "1004\n",
      "-----------------------------------------------------\n",
      "1006\n",
      "-----------------------------------------------------\n",
      "1412\n",
      "-----------------------------------------------------\n",
      "1412\n",
      "-----------------------------------------------------\n",
      "797\n",
      "-----------------------------------------------------\n",
      "643\n",
      "-----------------------------------------------------\n",
      "733\n",
      "-----------------------------------------------------\n",
      "792\n",
      "-----------------------------------------------------\n",
      "792\n",
      "-----------------------------------------------------\n",
      "518\n",
      "-----------------------------------------------------\n",
      "518\n",
      "-----------------------------------------------------\n",
      "634\n",
      "-----------------------------------------------------\n",
      "634\n",
      "-----------------------------------------------------\n",
      "519\n",
      "-----------------------------------------------------\n",
      "519\n",
      "-----------------------------------------------------\n",
      "685\n",
      "-----------------------------------------------------\n",
      "685\n",
      "-----------------------------------------------------\n",
      "611\n",
      "-----------------------------------------------------\n",
      "611\n",
      "-----------------------------------------------------\n",
      "676\n",
      "-----------------------------------------------------\n",
      "676\n",
      "-----------------------------------------------------\n",
      "239\n",
      "-----------------------------------------------------\n",
      "239\n",
      "-----------------------------------------------------\n",
      "215\n",
      "-----------------------------------------------------\n",
      "215\n",
      "-----------------------------------------------------\n",
      "861\n",
      "-----------------------------------------------------\n",
      "332\n",
      "-----------------------------------------------------\n",
      "332\n",
      "-----------------------------------------------------\n",
      "1073\n",
      "-----------------------------------------------------\n",
      "1073\n",
      "-----------------------------------------------------\n",
      "880\n",
      "-----------------------------------------------------\n",
      "696\n",
      "-----------------------------------------------------\n",
      "806\n",
      "-----------------------------------------------------\n",
      "478\n",
      "-----------------------------------------------------\n",
      "481\n",
      "-----------------------------------------------------\n",
      "383\n",
      "-----------------------------------------------------\n",
      "383\n",
      "-----------------------------------------------------\n",
      "240\n",
      "-----------------------------------------------------\n",
      "240\n",
      "-----------------------------------------------------\n",
      "354\n",
      "-----------------------------------------------------\n",
      "354\n",
      "-----------------------------------------------------\n",
      "472\n",
      "-----------------------------------------------------\n",
      "472\n",
      "-----------------------------------------------------\n",
      "235\n",
      "-----------------------------------------------------\n",
      "167\n",
      "-----------------------------------------------------\n",
      "167\n",
      "-----------------------------------------------------\n",
      "1090\n",
      "-----------------------------------------------------\n",
      "647\n",
      "-----------------------------------------------------\n",
      "655\n",
      "-----------------------------------------------------\n",
      "896\n",
      "-----------------------------------------------------\n",
      "395\n",
      "-----------------------------------------------------\n",
      "395\n",
      "-----------------------------------------------------\n",
      "205\n",
      "-----------------------------------------------------\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,100):\n",
    "    print('-----------------------------------------------------')\n",
    "    code = get_GCJ_code(sample_number=i)\n",
    "\n",
    "    new_code = code.replace(' ',' SPACE')\n",
    "    #newline\n",
    "    new_code = new_code.replace('\\n',' NEWLINE')\n",
    "    #tab\n",
    "    new_code = new_code.replace('\\t',' TAB')\n",
    "\n",
    "    tokens = tokenizer.encode(new_code,t=str)\n",
    "\n",
    "\n",
    "    wd=20 #window diameter\n",
    "    X_windows = centered_sliding_window(tokens,wd,encode=True)\n",
    "    code, breaks = get_predicted_break_points(X_windows,LSTM_LM_net_trained)\n",
    "    \n",
    "    if len(breaks)>0:\n",
    "        print(i,len(breaks))\n",
    "        '''\n",
    "        #from code segmentation file\n",
    "        comments_added = insert_comments(code,breaks)\n",
    "        comments_added_decoded = tokenizer.decode(comments_added)\n",
    "        comments_added_token_string = ''.join(comments_added_decoded)\n",
    "        comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "        comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "        comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "        print(comments_added_token_string)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on target MSC viaa dataloader for exact accuracy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(source, target):\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "    \n",
    "    \n",
    "    #SHOW RESUTLS on all Tokens\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #accuracy\n",
    "        acc = metrics.accuracy_score(y, numpy.rint(y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y, y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    #instantiate tokenizer model\n",
    "    \n",
    "    class Tokenizer:\n",
    "    \n",
    "        def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "            self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "        def encode(self, text, t=int):\n",
    "            return self.sp.encode(text, out_type=t)\n",
    "\n",
    "        def decode(self, pieces):\n",
    "            return self.sp.decode(pieces)\n",
    "\n",
    "        @staticmethod\n",
    "        def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "            spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                           #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                           input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer(name+'_tokenizer.model')\n",
    "    \n",
    "    \n",
    "    #make a _NEW_LINE_ ROC curve since thats what we care about\n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #filter for only new line places (get info from batch['x'])\n",
    "        new_y=[]\n",
    "        new_y_preds=[]\n",
    "        tokens = torch.flatten(batch['x']).detach().numpy()\n",
    "        for i in range(len(tokens)):\n",
    "            tok_literal=tokenizer.decode(int(tokens[i]))\n",
    "            if tok_literal[-7:]=='NEWLINE':\n",
    "                #only rate actual break places\n",
    "                #if y[i]==1:\n",
    "                #    new_y.append(y[i])\n",
    "                #    new_y_preds.append(y_preds[i])\n",
    "                new_y.append(y[i])\n",
    "                new_y_preds.append(y_preds[i])\n",
    "        #accuracy\n",
    "        #https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        acc = metrics.accuracy_score(new_y, numpy.rint(new_y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(new_y, new_y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
