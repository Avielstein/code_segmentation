{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from Chap 6\n",
    "#also could be useful\n",
    "#https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random as ra\n",
    "import numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "exec(open(\"01-utilities.py\").read())\n",
    "exec(open(\"05-utilities.py\").read())\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "def get_segments(name):\n",
    "    #filename= './'+name+'_LSTM_DATA.json'\n",
    "    #segments = json.load(open('./LSTM_DATA.json'))\n",
    "    \n",
    "    segments = json.load(open('./'+name+'_LSTM_DATA.json'))\n",
    "\n",
    "    #print data info\n",
    "    print('number of code snippet samples:',len(segments))\n",
    "    #total samples wtihin snippets\n",
    "    count = 0\n",
    "    #find number of samples in a snippet\n",
    "    for s in segments:\n",
    "        count+=len(segments[s]['x'])\n",
    "\n",
    "    print('number of total samples from parsed snippets:',count)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "###???\n",
    "#l = np.random.choice(range(0,100),4)\n",
    "#l[0] in l\n",
    "\n",
    "\n",
    "'''\n",
    "def get_sampled_indexes(segment_length,num,frac=False):\n",
    "    if frac:\n",
    "        sampled_indexes = np.random.choice(range(0,segment_length),int(segment_length/num))\n",
    "    else:\n",
    "        sampled_indexes = np.random.choice(range(0,segment_length),num)\n",
    "        \n",
    "    return sampled_indexes\n",
    "'''\n",
    "def get_sampled_indexes(segment_length,num,frac=False):\n",
    "    if frac:\n",
    "        sampled_indexes = ra.sample(range(0,segment_length),int(segment_length/num))\n",
    "    else:\n",
    "        sampled_indexes = ra.sample(range(0,segment_length),num)\n",
    "        \n",
    "    return sampled_indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split the data\n",
    "\n",
    "#'''\n",
    "\n",
    "def get_samples(segments, num_train, num_val, num_test, mult = 1 ):\n",
    "    \n",
    "    #multi serves to increase the number of samples and number sampled from data\n",
    "    \n",
    "    \n",
    "    train_x=[]\n",
    "    train_y=[]\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    for s in segments:\n",
    "\n",
    "\n",
    "        #only use X amount of data from each segment\n",
    "        #to reduce data size and train on more samples (so more representive of whole data set)\n",
    "        segment_length = len(segments[s]['x'])\n",
    "        sampled_indexes = get_sampled_indexes(segment_length,mult)\n",
    "\n",
    "        for ix,x in enumerate(segments[s]['x']):\n",
    "\n",
    "            y = segments[s]['y'][ix]\n",
    "\n",
    "            #must be of correct shape to be used\n",
    "            if len(x)==41 and len(y)==41 and ix in sampled_indexes: #and 1 in y:\n",
    "                if len(train_x)<num_train*mult:\n",
    "                    train_x.append(x)\n",
    "                    train_y.append(y)\n",
    "                elif len(val_x)<num_val*mult:\n",
    "                    val_x.append(x)\n",
    "                    val_y.append(y)\n",
    "                elif len(test_x)<num_test*mult:\n",
    "                    test_x.append(x)\n",
    "                    test_y.append(y)\n",
    "                else:\n",
    "                    #extra data\n",
    "                    pass\n",
    "                \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "#'''\n",
    "\n",
    "class code_snippet_loader(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset wrapper for the code segmentation dataset as an LM!\n",
    "    \"\"\"\n",
    "    def __init__(self,x,y):\n",
    "        # set up our data containers \n",
    "        # note: we will be creating a list of tensors\n",
    "        self.x = torch.LongTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # the length of our dataset is simply \n",
    "        # the number of samples!\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # when calling a specific example,\n",
    "        # we return the input, which is also the gold label!\n",
    "        return {'x': self.x[item],\n",
    "                'y': self.y[item]}\n",
    "    \n",
    "def train_lm(model, train_data, val_data, \n",
    "             word_vectors, num_train, num_val, name,\n",
    "             batch_size=50, lr=0.01, mu=0.25):\n",
    "    # setup our optimizer and loss function\n",
    "    opt = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    #loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    torch.manual_seed(691)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch = 0\n",
    "    no_imp = 0\n",
    "    patience=5\n",
    "    \n",
    "    # max_epoch turns off early stopping, when set\n",
    "    max_epochs = 250 #100 #50\n",
    "    training = True\n",
    "    best_val_loss = None\n",
    "    \n",
    "    history = {}\n",
    "    \n",
    "    training=True\n",
    "\n",
    "    #while (training and not max_epoch) or (epoch < max_epoch):\n",
    "    #while training and epoch < max_epoch:\n",
    "    while epoch < max_epochs and no_imp < patience:\n",
    "        print(f'Start epoch {epoch + 1}')\n",
    "\n",
    "        # Turn on training mode which enables dropout.\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        vocab_size = word_vectors.shape[0]\n",
    "        #print('----')\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            ###\n",
    "            ###MAKE SOME PRINTOUTS\n",
    "            ###\n",
    "            # clear any remaining gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            y_preds, h = model(to_gpu(batch['x']))\n",
    "            \n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fn(y_preds.view(-1), to_gpu(batch['y'].view(-1)))\n",
    "            train_loss += loss.cpu().item()\n",
    "            #print(train_loss)\n",
    "            \n",
    "            # Calculate the gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # `clip_grad_norm` applies gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), mu)\n",
    "\n",
    "            # update the parameteres yet after clipping\n",
    "            opt.step()\n",
    "\n",
    "        print(f'Current train loss: {train_loss:.2f}')\n",
    "        print(f'Average per sample train loss: {(train_loss/num_train):.2f}')\n",
    "\n",
    "        # disable gradients during validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        for batch in val_loader:\n",
    "            y_preds, h = model(to_gpu(batch['x']))\n",
    "            val_loss += loss_fn(y_preds.view(-1), to_gpu(batch['y'].view(-1))).cpu().item()\n",
    "\n",
    "        print(f'Total validation loss: {val_loss:.2f}')\n",
    "        print(f'Average per sample validation loss: {(val_loss/num_val):.2f}')\n",
    "        \n",
    "        if best_val_loss is None or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_imp = 0\n",
    "            torch.save(model.state_dict(), './data/'+name+'biLSTM_LM.pt')\n",
    "        else:\n",
    "            no_imp += 1\n",
    "\n",
    "        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n",
    "        \n",
    "        '''\n",
    "        if not best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        elif val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            print('No improvement! Early stopping')\n",
    "            training = False\n",
    "        '''\n",
    "\n",
    "    \n",
    "        epoch += 1\n",
    "        \n",
    "        history['epoch_'+str(epoch)]={'train_loss':train_loss,'val_loss':val_loss}\n",
    "        print()\n",
    "\n",
    "    return model,history\n",
    "\n",
    "\n",
    "###\n",
    "# adapted from the PyTorch examples. for the full PyTorch LM example, see: \n",
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "###\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    \"\"\"Model feeds pre-trained embeddings through a series of biLSTM\n",
    "       layers, followed by a linear vocabulary decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, lstm_layers, word_vectors, \n",
    "                 dropout=0.05, bidirectional = True):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = word_vectors.shape[0]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        # blank embed layer starting from GloVe pre-trained vectors\n",
    "        self._embed = nn.Embedding.from_pretrained(word_vectors, freeze=False)        \n",
    "        self._drop = nn.Dropout(dropout)\n",
    "\n",
    "        self._lstm = nn.LSTM(in_dim, hidden_dim, num_layers = lstm_layers, dropout = dropout,\n",
    "                             bidirectional = bidirectional, batch_first=True)\n",
    "        self._ReLU = nn.ReLU()\n",
    "        self._pred = nn.Linear((2 if bidirectional else 1)*hidden_dim, \n",
    "                               #self.vocab_size)\n",
    "                               1) #only 1 or zeros here \n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self._drop(self._embed(x))\n",
    "        z, h = self._lstm(e)\n",
    "        z_drop = self._drop(z)\n",
    "        s = self._pred(self._ReLU(z_drop))\n",
    "        #s = s.view(-1, self.vocab_size)\n",
    "        s = s.squeeze()\n",
    "        return s, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.lstm_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_code_segmentation_model(name):\n",
    "    \n",
    "    #add name param\n",
    "    segments = get_segments(name)\n",
    "\n",
    "    num_train = 5000\n",
    "    num_val   = 1500\n",
    "    num_test  = 1500\n",
    "\n",
    "    #split data\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_samples(segments, num_train, num_val, num_test)\n",
    "\n",
    "\n",
    "    print('Data Set Up')\n",
    "    print('number of training samples:',len(train_x))\n",
    "    print('number of validation samples:',len(val_x))\n",
    "    print('number of testing samples:',len(test_x))\n",
    "\n",
    "    LM_train_loader = code_snippet_loader(train_x, train_y)\n",
    "    LM_val_loader = code_snippet_loader(val_x, val_y)\n",
    "    LM_test_loader = code_snippet_loader(test_x, test_y)\n",
    "\n",
    "    # considering what our new input, target pairs look like:\n",
    "    print(len(LM_train_loader.x), LM_train_loader[3])\n",
    "\n",
    "\n",
    "    torch.manual_seed(691)\n",
    "\n",
    "    #vocab size from sentence peice\n",
    "    #vocab dim???? \n",
    "    vocab_size = 10000 #same as sentence peice\n",
    "    vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "    # randomly initialize our word vectors!\n",
    "    vocab_dim = 256\n",
    "    word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "    word_vectors.shape, word_vectors\n",
    "\n",
    "    #show model\n",
    "    hidden_dim = 200\n",
    "    lstm_layers = 2\n",
    "    #LSTM_LM_net = to_gpu(LSTM_LM(newstweet_wvs.shape[1], hidden_dim,lstm_layers, newstweet_wvs))\n",
    "    LSTM_LM_net = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "    LSTM_LM_net\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    #batch_size = 50 \n",
    "    batch_size = 50\n",
    "\n",
    "    # an initial learning rate, prior to clipping\n",
    "    lr = 0.0005\n",
    "    #lr = 0.01\n",
    "\n",
    "    # the clipping theshold\n",
    "    mu = 0.25\n",
    "\n",
    "    LSTM_LM_net_trained, history = train_lm(LSTM_LM_net, LM_train_loader, LM_val_loader, \n",
    "                                            word_vectors, num_train, num_val, name,\n",
    "                                            batch_size=batch_size, lr=lr, mu=mu)\n",
    "\n",
    "\n",
    "    #show \n",
    "\n",
    "    training_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    for loss in history:\n",
    "\n",
    "        #normal\n",
    "        #training_loss_values.append(history[loss]['train_loss'])\n",
    "        #validation_loss_values.append(history[loss]['val_loss'])\n",
    "        #\n",
    "        #adjusted normal\n",
    "        training_loss_values.append(history[loss]['train_loss']/num_train)\n",
    "        validation_loss_values.append(history[loss]['val_loss']/num_val)\n",
    "        #\n",
    "        #log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']))\n",
    "        #\n",
    "        #adjusted log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']/num_train))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']/num_val))\n",
    "\n",
    "\n",
    "    Epochs = range(len(training_loss_values))\n",
    "\n",
    "\n",
    "    #show all\n",
    "    plt.plot(Epochs, training_loss_values)\n",
    "    plt.plot(Epochs, validation_loss_values)\n",
    "    #show all but the first one\n",
    "    #plt.plot(Epochs[1:], training_loss_values[1:])\n",
    "    #plt.plot(Epochs[1:], validation_loss_values[1:])\n",
    "    plt.title('Average loss per sample')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "    \n",
    "    \n",
    "    #SHOW RESUTLS on all Tokens\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #accuracy\n",
    "        acc = metrics.accuracy_score(y, numpy.rint(y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y, y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    #instantiate tokenizer model\n",
    "    \n",
    "    class Tokenizer:\n",
    "    \n",
    "        def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "            self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "        def encode(self, text, t=int):\n",
    "            return self.sp.encode(text, out_type=t)\n",
    "\n",
    "        def decode(self, pieces):\n",
    "            return self.sp.decode(pieces)\n",
    "\n",
    "        @staticmethod\n",
    "        def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "            spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                           #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                           input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer(name+'_tokenizer.model')\n",
    "    \n",
    "    \n",
    "    #make a _NEW_LINE_ ROC curve since thats what we care about\n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #filter for only new line places (get info from batch['x'])\n",
    "        new_y=[]\n",
    "        new_y_preds=[]\n",
    "        tokens = torch.flatten(batch['x']).detach().numpy()\n",
    "        for i in range(len(tokens)):\n",
    "            tok_literal=tokenizer.decode(int(tokens[i]))\n",
    "            if tok_literal[-7:]=='NEWLINE':\n",
    "                #only rate actual break places\n",
    "                #if y[i]==1:\n",
    "                #    new_y.append(y[i])\n",
    "                #    new_y_preds.append(y_preds[i])\n",
    "                new_y.append(y[i])\n",
    "                new_y_preds.append(y_preds[i])\n",
    "        #accuracy\n",
    "        #https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        acc = metrics.accuracy_score(new_y, numpy.rint(new_y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(new_y, new_y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "        \n",
    "#train_code_segmentation_model('py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "trianing... py short\n",
      "--------------------------------------------------\n",
      "number of code snippet samples: 10000\n",
      "number of total samples from parsed snippets: 2077520\n",
      "Data Set Up\n",
      "number of training samples: 5000\n",
      "number of validation samples: 1500\n",
      "number of testing samples: 1500\n",
      "5000 {'x': tensor([  3,   3,   3,  11,   3,   5,   5,  63,  29,   6,  10,   3,  93,   3,\n",
      "         14,   3, 734,   9,   4,   3,   3,   3,   3,   3,   3,   3,   3,   6,\n",
      "          8,  93,   3,  14,   3, 271,   8, 271,   7,  93,  13,   4,   4]), 'y': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "Start epoch 1\n",
      "Current train loss: 22709.63\n",
      "Average per sample train loss: 4.54\n",
      "Total validation loss: 2442.28\n",
      "Average per sample validation loss: 1.63\n",
      "Best validation loss: 2442.28 (Epochs without improvement: 0)\n",
      "\n",
      "Start epoch 2\n",
      "Current train loss: 6163.70\n",
      "Average per sample train loss: 1.23\n",
      "Total validation loss: 1399.69\n",
      "Average per sample validation loss: 0.93\n",
      "Best validation loss: 1399.69 (Epochs without improvement: 0)\n",
      "\n",
      "Start epoch 3\n",
      "Current train loss: 3936.71\n",
      "Average per sample train loss: 0.79\n",
      "Total validation loss: 950.34\n",
      "Average per sample validation loss: 0.63\n",
      "Best validation loss: 950.34 (Epochs without improvement: 0)\n",
      "\n",
      "Start epoch 4\n",
      "Current train loss: 2757.01\n",
      "Average per sample train loss: 0.55\n",
      "Total validation loss: 678.55\n",
      "Average per sample validation loss: 0.45\n",
      "Best validation loss: 678.55 (Epochs without improvement: 0)\n",
      "\n",
      "Start epoch 5\n",
      "Current train loss: 2126.66\n",
      "Average per sample train loss: 0.43\n",
      "Total validation loss: 549.05\n",
      "Average per sample validation loss: 0.37\n",
      "Best validation loss: 549.05 (Epochs without improvement: 0)\n",
      "\n",
      "Start epoch 6\n"
     ]
    }
   ],
   "source": [
    "#single language, all sizes\n",
    "#'''\n",
    "for i in ['py']:#,'cpp','java']:\n",
    "    for j in ['short','medium','long']:\n",
    "        print('--------------------------------------------------')\n",
    "        print('trianing...',i,j)\n",
    "        print('--------------------------------------------------')\n",
    "        name = j+'_'+i\n",
    "        train_code_segmentation_model(name)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the biLSTM's weights\n",
    "# ...we now have save best in training\n",
    "#torch.save(LSTM_LM_net_trained.state_dict(), './data/biLSTM_LM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO FIX TO WORK FOR LONG SHORT AND MEDIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multilingual_code_segmentation_model(size):\n",
    "    \n",
    "    \n",
    "    name = size+'_all'\n",
    "    num_train = 1500\n",
    "    num_val   = 500\n",
    "    num_test  = 500\n",
    "    \n",
    "    \n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    \n",
    "    languages = ['py','cpp','java']\n",
    "    for l in languages:\n",
    "        segments = get_segments(size+'_'+l)\n",
    "        #split data\n",
    "        train_x_temp, train_y_temp, val_x_temp, val_y_temp, test_x_temp, test_y_temp = get_samples(segments, num_train, num_val, num_test)\n",
    "        \n",
    "        train_x+=train_x_temp\n",
    "        train_y+=train_y_temp\n",
    "        val_x+=val_x_temp\n",
    "        val_y+=val_y_temp\n",
    "        test_x+=test_x_temp\n",
    "        test_y+=test_y_temp\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    print('Data Set Up')\n",
    "    print('number of training samples:',len(train_x))\n",
    "    print('number of validation samples:',len(val_x))\n",
    "    print('number of testing samples:',len(test_x))\n",
    "\n",
    "    LM_train_loader = code_snippet_loader(train_x, train_y)\n",
    "    LM_val_loader = code_snippet_loader(val_x, val_y)\n",
    "    LM_test_loader = code_snippet_loader(test_x, test_y)\n",
    "\n",
    "    # considering what our new input, target pairs look like:\n",
    "    print(len(LM_train_loader.x), LM_train_loader[3])\n",
    "\n",
    "\n",
    "    torch.manual_seed(691)\n",
    "\n",
    "    #vocab size from sentence peice\n",
    "    #vocab dim???? \n",
    "    vocab_size = 10000 #same as sentence peice\n",
    "    vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "    # randomly initialize our word vectors!\n",
    "    vocab_dim = 256\n",
    "    word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "    word_vectors.shape, word_vectors\n",
    "\n",
    "    #show model\n",
    "    hidden_dim = 200\n",
    "    lstm_layers = 2\n",
    "    #LSTM_LM_net = to_gpu(LSTM_LM(newstweet_wvs.shape[1], hidden_dim,lstm_layers, newstweet_wvs))\n",
    "    LSTM_LM_net = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "    LSTM_LM_net\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    #batch_size = 50 \n",
    "    batch_size = 50\n",
    "\n",
    "    # an initial learning rate, prior to clipping\n",
    "    lr = 0.0005\n",
    "    #lr = 0.01\n",
    "\n",
    "    # the clipping theshold\n",
    "    mu = 0.25\n",
    "\n",
    "    LSTM_LM_net_trained, history = train_lm(LSTM_LM_net, LM_train_loader, LM_val_loader, \n",
    "                                            word_vectors, num_train, num_val, name,\n",
    "                                            batch_size=batch_size, lr=lr, mu=mu)\n",
    "\n",
    "\n",
    "    #show \n",
    "\n",
    "    training_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    for loss in history:\n",
    "\n",
    "        #normal\n",
    "        #training_loss_values.append(history[loss]['train_loss'])\n",
    "        #validation_loss_values.append(history[loss]['val_loss'])\n",
    "        #\n",
    "        #adjusted normal\n",
    "        training_loss_values.append(history[loss]['train_loss']/num_train)\n",
    "        validation_loss_values.append(history[loss]['val_loss']/num_val)\n",
    "        #\n",
    "        #log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']))\n",
    "        #\n",
    "        #adjusted log\n",
    "        #training_loss_values.append(np.log(history[loss]['train_loss']/num_train))\n",
    "        #validation_loss_values.append(np.log(history[loss]['val_loss']/num_val))\n",
    "\n",
    "\n",
    "    Epochs = range(len(training_loss_values))\n",
    "\n",
    "\n",
    "    #show all\n",
    "    plt.plot(Epochs, training_loss_values)\n",
    "    plt.plot(Epochs, validation_loss_values)\n",
    "    #show all but the first one\n",
    "    #plt.plot(Epochs[1:], training_loss_values[1:])\n",
    "    #plt.plot(Epochs[1:], validation_loss_values[1:])\n",
    "    plt.title('Average loss per sample')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "    \n",
    "    \n",
    "    #SHOW RESUTLS on all Tokens\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #accuracy\n",
    "        acc = metrics.accuracy_score(y, numpy.rint(y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y, y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    #instantiate tokenizer model\n",
    "    \n",
    "    class Tokenizer:\n",
    "    \n",
    "        def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "            self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "        def encode(self, text, t=int):\n",
    "            return self.sp.encode(text, out_type=t)\n",
    "\n",
    "        def decode(self, pieces):\n",
    "            return self.sp.decode(pieces)\n",
    "\n",
    "        @staticmethod\n",
    "        def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "            spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                           #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                           input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "    \n",
    "    tokenizer = Tokenizer(name+'_tokenizer.model')\n",
    "    \n",
    "    \n",
    "    #make a _NEW_LINE_ ROC curve since thats what we care about\n",
    "    test_loader = torch.utils.data.DataLoader(LM_test_loader, batch_size=num_test, shuffle=True)\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    for batch in test_loader:\n",
    "\n",
    "        #get predicted values\n",
    "        y_preds, h = LSTM_LM_net_trained(batch['x'])\n",
    "        y_preds = torch.flatten(torch.sigmoid(y_preds))\n",
    "        y_preds= y_preds.detach().numpy()\n",
    "\n",
    "        #get true values\n",
    "        y = batch['y'].view(-1)\n",
    "        y = y.detach().numpy()\n",
    "\n",
    "        #filter for only new line places (get info from batch['x'])\n",
    "        new_y=[]\n",
    "        new_y_preds=[]\n",
    "        tokens = torch.flatten(batch['x']).detach().numpy()\n",
    "        for i in range(len(tokens)):\n",
    "            tok_literal=tokenizer.decode(int(tokens[i]))\n",
    "            if tok_literal[-7:]=='NEWLINE':\n",
    "                #only rate actual break places\n",
    "                #if y[i]==1:\n",
    "                #    new_y.append(y[i])\n",
    "                #    new_y_preds.append(y_preds[i])\n",
    "                new_y.append(y[i])\n",
    "                new_y_preds.append(y_preds[i])\n",
    "        #accuracy\n",
    "        #https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        acc = metrics.accuracy_score(new_y, numpy.rint(new_y_preds))\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr, tpr, threshold = metrics.roc_curve(new_y, new_y_preds)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print('Accuracy:',acc)\n",
    "    print('Area under curve:',roc_auc)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) on Test Set')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "for s in ['short','medium','long']:\n",
    "    train_multilingual_code_segmentation_model(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
