{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a program from google code jam\n",
    "#load segmentation LSTM\n",
    "#process program to be read by LSTM\n",
    "#use LSTM to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import random as ra\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from code segmentation file\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, filepath='python_tokenizer_30k.model'):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                       #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                       input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "        \n",
    "        #instantiate tokenizer model\n",
    "tokenizer = Tokenizer('python_tokenizer.model')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# adapted from the PyTorch examples. for the full PyTorch LM example, see: \n",
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "###\n",
    "class LSTM_LM(nn.Module):\n",
    "    \"\"\"Model feeds pre-trained embeddings through a series of biLSTM\n",
    "       layers, followed by a linear vocabulary decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, lstm_layers, word_vectors, \n",
    "                 dropout=0.05, bidirectional = True):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = word_vectors.shape[0]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        # blank embed layer starting from GloVe pre-trained vectors\n",
    "        self._embed = nn.Embedding.from_pretrained(word_vectors, freeze=False)        \n",
    "        self._drop = nn.Dropout(dropout)\n",
    "\n",
    "        self._lstm = nn.LSTM(in_dim, hidden_dim, num_layers = lstm_layers, dropout = dropout,\n",
    "                             bidirectional = bidirectional, batch_first=True)\n",
    "        self._ReLU = nn.ReLU()\n",
    "        self._pred = nn.Linear((2 if bidirectional else 1)*hidden_dim, \n",
    "                               #self.vocab_size)\n",
    "                               1) #only 1 or zeros here \n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self._drop(self._embed(x))\n",
    "        z, h = self._lstm(e)\n",
    "        z_drop = self._drop(z)\n",
    "        s = self._pred(self._ReLU(z_drop))\n",
    "        #s = s.view(-1, self.vocab_size)\n",
    "        s = s.squeeze()\n",
    "        return s, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.lstm_layers, batch_size, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_LM(\n",
       "  (_embed): Embedding(10000, 256)\n",
       "  (_drop): Dropout(p=0.05, inplace=False)\n",
       "  (_lstm): LSTM(256, 200, num_layers=2, batch_first=True, dropout=0.05, bidirectional=True)\n",
       "  (_ReLU): ReLU()\n",
       "  (_pred): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "## use saved model\n",
    "#######\n",
    "\n",
    "torch.manual_seed(691)\n",
    "\n",
    "#vocab size from sentence peice\n",
    "#vocab dim???? \n",
    "vocab_size = 10000 #same as sentence peice\n",
    "vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "# randomly initialize our word vectors!\n",
    "vocab_dim = 256\n",
    "word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "word_vectors.shape, word_vectors\n",
    "\n",
    "#set up model\n",
    "hidden_dim = 200\n",
    "lstm_layers = 2\n",
    "LSTM_LM_net_trained = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "#[TODO]: fix so it works\n",
    "#https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "#https://stackoverflow.com/questions/61242966/pytorch-attributeerror-function-object-has-no-attribute-copy\n",
    "\n",
    "#load weights into model\n",
    "LSTM_LM_net_trained.load_state_dict(torch.load('data/long_pybiLSTM_LM.pt'))\n",
    "LSTM_LM_net_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to\n",
    "# demonstrate readlines()\n",
    "#get all python files and associated task\n",
    "def get_GCJ_code():\n",
    "    path = 'segment_program.py'\n",
    "    code = ''\n",
    "    # Using readlines()\n",
    "    file1 = open(path, 'r')\n",
    "    Lines = file1.readlines()\n",
    " \n",
    "    count = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        code+=line\n",
    "    return code\n",
    "\n",
    "#get_GCJ_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centered_sliding_window(token_list, window_diamiter,encode=False,PAD='unk'):\n",
    "    windows = []\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        #print(token_list)\n",
    "        #input()\n",
    "        \n",
    "        window = []\n",
    "        \n",
    "        #if we have to pad the begining\n",
    "        if i < window_diamiter:\n",
    "            before_len = window_diamiter-i\n",
    "            before = [PAD]*before_len+token_list[0:i]\n",
    "        else:\n",
    "            before = token_list[i-window_diamiter:i]\n",
    "        \n",
    "        #if we have to pad the end\n",
    "        if i+window_diamiter>=len(token_list):\n",
    "            after_len = (i+1+window_diamiter)-len(token_list)\n",
    "            after = token_list[i+1:i+1+window_diamiter]+[PAD]*after_len\n",
    "\n",
    "        else:\n",
    "            after = token_list[i+1:i+1+window_diamiter]\n",
    "        \n",
    "        #put it togeather\n",
    "        #print('------')\n",
    "        #print('before:',before)\n",
    "        #print('center:',token_list[i])\n",
    "        #print('after:',after)\n",
    "        window = before + [token_list[i]] + after\n",
    "        #for encoding code if we want\n",
    "        if encode:\n",
    "            new_window = []\n",
    "            #print(window)\n",
    "            #input()\n",
    "            for i in window:\n",
    "                encoded = tokenizer.encode(i)\n",
    "                if len(encoded)>1:       \n",
    "                    x=encoded[1]\n",
    "                    if type(x)==list:\n",
    "                        new_window.append(x[0])\n",
    "                    else:\n",
    "                        new_window.append(x)\n",
    "                elif len(encoded)==1:\n",
    "                    if type(encoded)==list:\n",
    "                        new_window.append(encoded[0])\n",
    "                    else:\n",
    "                        new_window.append(encoded)\n",
    "                else:\n",
    "                    #for some reason it finds the unicode stuff __\n",
    "                    pass\n",
    "                    #print(window)\n",
    "                    #print(i)\n",
    "                    #print(encoded)\n",
    "                    #input()\n",
    "            #print(window)\n",
    "            #print(len(window))\n",
    "            #print(len(tokenizer.decode(window)))\n",
    "            #print(tokenizer.decode(window))\n",
    "            #print(len(tokenizer.encode(window)))\n",
    "            #window = tokenizer.encode(tokenizer.decode(window))\n",
    "            window = new_window\n",
    "        #print(window)\n",
    "        #print(len(window))\n",
    "        #input()\n",
    "\n",
    "        #save windowz\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#NOTE, THIS ONLY GETS THE PREDICTED BREAK POINTS FROM A PREDICTION\n",
    "#WITH THE NEWLINE TOKEN AT CENTER OF WINDOW\n",
    "def get_predicted_break_points(code_windows, model):\n",
    "    start = 0\n",
    "    code  =[]\n",
    "    break_points = []\n",
    "    print(len(code_windows))\n",
    "    for window_i in range(len(code_windows)):\n",
    "        #get window, which has our tokens\n",
    "        window = code_windows[window_i]\n",
    "        window_predictions = get_window_predictions(window,model)\n",
    "        #mid = math.ceil(len(window)/2)\n",
    "        mid = int(len(window)/2) #actually we need to round down...\n",
    "        mid_token = tokenizer.decode(int(window[mid]))\n",
    "        mid_pred = window_predictions[mid]\n",
    "        if mid_token[-7:]=='NEWLINE':\n",
    "            if round(mid_pred) == 1:\n",
    "                break_points.append(window_i)\n",
    "                \n",
    "                \n",
    "        code.append(mid_token)\n",
    "        start+=1\n",
    "    return code, break_points\n",
    "    \n",
    "#code_windows = segments['0']['x']\n",
    "#code, breaks = get_predicted_break_points(code_windows,LSTM_LM_net_trained)\n",
    "#print(breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all predictions from one window\n",
    "def get_window_predictions(window, model):\n",
    "    print(window)\n",
    "    preds, h = model(torch.tensor([window]))\n",
    "    preds = torch.flatten(torch.sigmoid(preds))\n",
    "    preds = preds.detach().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from code segmentation file\n",
    "def insert_comments(code, break_spots, comment='\\n'+'*'*8+'\\n',at_begining=True):\n",
    "    #if there is a a comment at begining of snippet\n",
    "    if at_begining:\n",
    "        #adds a notation to add a 0\n",
    "        #at beigning of break spots too\n",
    "        break_spots.insert(0,0)\n",
    "    \n",
    "    #go through breaks backwards\n",
    "    #so as not to mess up break \n",
    "    #spots as we would if we went forward\n",
    "    for b in break_spots[::-1]:\n",
    "        code.insert(b,comment)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751\n",
      "[6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 6754, 7425, 14650, 1900, 1900, 4, 36, 3, 14527, 4, 14650, 7425, 1900, 1900, 4, 4, 4, 36, 21751, 11379, 4]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-44fc4732de5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_added_token_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-44fc4732de5b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;31m#window diameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mX_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentered_sliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreaks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predicted_break_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_windows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLSTM_LM_net_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#from code segmentation file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-023ea03e4fe1>\u001b[0m in \u001b[0;36mget_predicted_break_points\u001b[0;34m(code_windows, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#get window, which has our tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_windows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mwindow_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_window_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#mid = math.ceil(len(window)/2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#actually we need to round down...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-eacc826a3fda>\u001b[0m in \u001b[0;36mget_window_predictions\u001b[0;34m(window, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_window_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e32a80da986>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mz_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    code = get_GCJ_code()\n",
    "    \n",
    "    \n",
    "    new_code = code.replace(' ',' SPACE')\n",
    "    #newline\n",
    "    new_code = new_code.replace('\\n',' NEWLINE')\n",
    "    #tab\n",
    "    new_code = new_code.replace('\\t',' TAB')\n",
    "\n",
    "    tokens = tokenizer.encode(new_code,t=str)\n",
    "\n",
    "\n",
    "    wd=20 #window diameter\n",
    "    X_windows = centered_sliding_window(tokens,wd,encode=True)\n",
    "    code, breaks = get_predicted_break_points(X_windows,LSTM_LM_net_trained)\n",
    "\n",
    "    #from code segmentation file\n",
    "    if len(breaks)>0:\n",
    "        print(i,len(breaks))\n",
    "        comments_added = insert_comments(code,breaks)\n",
    "        comments_added_decoded = tokenizer.decode(comments_added)\n",
    "        comments_added_token_string = ''.join(comments_added_decoded)\n",
    "        comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "        comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "        comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "        print(comments_added_token_string)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
