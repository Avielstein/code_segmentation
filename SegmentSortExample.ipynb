{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code():\n",
    "    code = '''from random import randint\n",
    "from timeit import repeat\n",
    "from random import randint\n",
    "\n",
    "def run_sorting_algorithm(algorithm, array):\n",
    "    setup_code = f\"from __main__ import {algorithm}\" \\\n",
    "        if algorithm != \"sorted\" else \"\"\n",
    "    stmt = f\"{algorithm}({array})\"\n",
    "    times = repeat(setup=setup_code, stmt=stmt, repeat=3, number=10)\n",
    "    print(f\"Algorithm: {algorithm}. Minimum execution time: {min(times)}\")\n",
    "\n",
    "def bubble_sort(array):\n",
    "    n = len(array)\n",
    "    for i in range(n):\n",
    "        already_sorted = True\n",
    "        for j in range(n - i - 1):\n",
    "            if array[j] > array[j + 1]:\n",
    "                array[j], array[j + 1] = array[j + 1], array[j]\n",
    "                already_sorted = False\n",
    "        if already_sorted:\n",
    "            break\n",
    "    return array\n",
    "\n",
    "def insertion_sort(array):\n",
    "    for i in range(1, len(array)):\n",
    "        key_item = array[i]\n",
    "        j = i - 1\n",
    "        while j >= 0 and array[j] > key_item:\n",
    "            array[j + 1] = array[j]\n",
    "            j -= 1\n",
    "        array[j + 1] = key_item\n",
    "    return array\n",
    "\n",
    "def merge(left, right):\n",
    "    if len(left) == 0:\n",
    "        return right\n",
    "    if len(right) == 0:\n",
    "        return left\n",
    "    result = []\n",
    "    index_left = index_right = 0\n",
    "    while len(result) < len(left) + len(right):\n",
    "        if left[index_left] <= right[index_right]:\n",
    "            result.append(left[index_left])\n",
    "            index_left += 1\n",
    "        else:\n",
    "            result.append(right[index_right])\n",
    "            index_right += 1\n",
    "        if index_right == len(right):\n",
    "            result += left[index_left:]\n",
    "            break\n",
    "        if index_left == len(left):\n",
    "            result += right[index_right:]\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def merge_sort(array):\n",
    "    # If the input array contains fewer than two elements,\n",
    "    # then return it as the result of the function\n",
    "    if len(array) < 2:\n",
    "        return array\n",
    "    midpoint = len(array) // 2\n",
    "    return merge(\n",
    "        left=merge_sort(array[:midpoint]),\n",
    "        right=merge_sort(array[midpoint:]))\n",
    "\n",
    "def quicksort(array):\n",
    "    if len(array) < 2:\n",
    "        return array\n",
    "    low, same, high = [], [], []\n",
    "    pivot = array[randint(0, len(array) - 1)]\n",
    "    for item in array:\n",
    "        if item < pivot:\n",
    "            low.append(item)\n",
    "        elif item == pivot:\n",
    "            same.append(item)\n",
    "        elif item > pivot:\n",
    "            high.append(item)\n",
    "    return quicksort(low) + same + quicksort(high)\n",
    "\n",
    "def insertion_sort(array, left=0, right=None):\n",
    "    if right is None:\n",
    "        right = len(array) - 1\n",
    "    for i in range(left + 1, right + 1):\n",
    "        key_item = array[i]\n",
    "        j = i - 1\n",
    "        while j >= left and array[j] > key_item:\n",
    "            array[j + 1] = array[j]\n",
    "            j -= 1\n",
    "        array[j + 1] = key_item\n",
    "    return array\n",
    "\n",
    "def timsort(array):\n",
    "    min_run = 32\n",
    "    n = len(array)\n",
    "    for i in range(0, n, min_run):\n",
    "        insertion_sort(array, i, min((i + min_run - 1), n - 1))\n",
    "    size = min_run\n",
    "    while size < n:\n",
    "        for start in range(0, n, size * 2):\n",
    "            midpoint = start + size - 1\n",
    "            end = min((start + size * 2 - 1), (n-1))\n",
    "            merged_array = merge(\n",
    "                left=array[start:midpoint + 1],\n",
    "                right=array[midpoint + 1:end + 1])\n",
    "            array[start:start + len(merged_array)] = merged_array\n",
    "        size *= 2\n",
    "    return array\n",
    "\n",
    "ARRAY_LENGTH = 1000\n",
    "if __name__ == \"__main__\":\n",
    "    array = [randint(0, 1000) for i in range(ARRAY_LENGTH)]\n",
    "    run_sorting_algorithm(algorithm=\"sorted\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"bubble_sort\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"insertion_sort\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"merge_sort\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"quicksort\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"insertion_sort\", array=array)\n",
    "    run_sorting_algorithm(algorithm=\"timsort\", array=array)'''\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code():\n",
    "    code = \"\"\"def quicksort(array):\n",
    "    if len(array) < 2:\n",
    "        return array\n",
    "    low, same, high = [], [], []\n",
    "    pivot = array[randint(0, len(array) - 1)]\n",
    "    for item in array:\n",
    "        if item < pivot:\n",
    "            low.append(item)\n",
    "        elif item == pivot:\n",
    "            same.append(item)\n",
    "        elif item > pivot:\n",
    "            high.append(item)\n",
    "    return quicksort(low) + same + quicksort(high)\"\"\"\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all predictions from one window\n",
    "def get_window_predictions(window, model):\n",
    "    preds, h = model(torch.tensor([window]))\n",
    "    preds = torch.flatten(torch.sigmoid(preds))\n",
    "    preds = preds.detach().numpy()\n",
    "    return preds\n",
    "\n",
    "#get_window_predictions(test_x[0],LSTM_LM_net_trained)\n",
    "\n",
    "\n",
    "\n",
    "#NOTE, THIS ONLY GETS THE PREDICTED BREAK POINTS FROM A PREDICTION\n",
    "#WITH THE NEWLINE TOKEN AT CENTER OF WINDOW\n",
    "#if top = 0, return all break points, else return top number of break points\n",
    "def get_predicted_break_points(code_windows, model, top=0, thresh=0.5):\n",
    "    start = 0\n",
    "    code  = []\n",
    "    break_points = []\n",
    "    mid_points_preds = []\n",
    "    print(len(code_windows))\n",
    "    for window_i in range(len(code_windows)):\n",
    "        #get window, which has our tokens\n",
    "        window = code_windows[window_i]\n",
    "        window_predictions = get_window_predictions(window,model)\n",
    "        #mid = math.ceil(len(window)/2)\n",
    "        mid = int(len(window)/2) #actually we need to round down...\n",
    "        mid_token = tokenizer.decode(int(window[mid]))\n",
    "        mid_pred = window_predictions[mid]\n",
    "        mid_points_preds.append(mid_pred)\n",
    "        \n",
    "        #only new lines\n",
    "        if mid_token[-7:]=='NEWLINE' and mid_pred >= thresh:\n",
    "            if top==0:\n",
    "                print(mid_token, mid_pred)\n",
    "                break_points.append(window_i)\n",
    "            else:\n",
    "                print('you forgot to fill this in ')\n",
    "                break_points.append(window_i)\n",
    "                \n",
    "                \n",
    "                \n",
    "        code.append(mid_token)\n",
    "        start+=1\n",
    "    \n",
    "    #print(max(mid_points_preds))\n",
    "    return code, break_points\n",
    "\n",
    "def get_top_n_preds(code_windows, model, top=3):\n",
    "    start = 0\n",
    "    code  = []\n",
    "    break_points = []\n",
    "    mid_points_preds = []\n",
    "    print(len(code_windows))\n",
    "    for window_i in tqdm(range(len(code_windows))):\n",
    "        #get window, which has our tokens\n",
    "        window = code_windows[window_i]\n",
    "        window_predictions = get_window_predictions(window,model)\n",
    "        #mid = math.ceil(len(window)/2)\n",
    "        mid = int(len(window)/2) #actually we need to round down...\n",
    "        mid_token = tokenizer.decode(int(window[mid]))\n",
    "        mid_pred = window_predictions[mid]\n",
    "        if mid_token[-7:]=='NEWLINE':\n",
    "            mid_points_preds.append(mid_pred)\n",
    "        else:\n",
    "            mid_points_preds.append(0)\n",
    "          \n",
    "        code.append(mid_token)\n",
    "        start+=1\n",
    "        \n",
    "    break_points = sorted(range(len(mid_points_preds)), key=lambda i: mid_points_preds[i])[-top:]\n",
    "    \n",
    "    #print(max(mid_points_preds))\n",
    "    return code, break_points\n",
    "\n",
    "\n",
    "    \n",
    "#code_windows = segments['0']['x']\n",
    "#code, breaks = get_predicted_break_points(code_windows,LSTM_LM_net_trained)\n",
    "#print(breaks)\n",
    "\n",
    "#from code segmentation file\n",
    "def insert_comments(code, break_spots, comment='\\n'+'*'*8+'\\n',at_begining=True):\n",
    "    #if there is a a comment at begining of snippet\n",
    "    if at_begining:\n",
    "        #adds a notation to add a 0\n",
    "        #at beigning of break spots too\n",
    "        break_spots.insert(0,0)\n",
    "    \n",
    "    #go through breaks backwards\n",
    "    #so as not to mess up break \n",
    "    #spots as we would if we went forward\n",
    "    for b in break_spots[::-1]:\n",
    "        code.insert(b,comment)\n",
    "    return code\n",
    "\n",
    "def centered_sliding_window(token_list, window_diamiter,encode=False,PAD='unk'):\n",
    "    windows = []\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        #print(token_list)\n",
    "        #input()\n",
    "        \n",
    "        window = []\n",
    "        \n",
    "        #if we have to pad the begining\n",
    "        if i < window_diamiter:\n",
    "            before_len = window_diamiter-i\n",
    "            before = [PAD]*before_len+token_list[0:i]\n",
    "        else:\n",
    "            before = token_list[i-window_diamiter:i]\n",
    "        \n",
    "        #if we have to pad the end\n",
    "        if i+window_diamiter>=len(token_list):\n",
    "            after_len = (i+1+window_diamiter)-len(token_list)\n",
    "            after = token_list[i+1:i+1+window_diamiter]+[PAD]*after_len\n",
    "\n",
    "        else:\n",
    "            after = token_list[i+1:i+1+window_diamiter]\n",
    "        \n",
    "        #put it togeather\n",
    "        #print('------')\n",
    "        #print('before:',before)\n",
    "        #print('center:',token_list[i])\n",
    "        #print('after:',after)\n",
    "        window = before + [token_list[i]] + after\n",
    "        #for encoding code if we want\n",
    "        if encode:\n",
    "            new_window = []\n",
    "            #print(window)\n",
    "            #input()\n",
    "            for i in window:\n",
    "                encoded = tokenizer.encode(i)\n",
    "                if len(encoded)>1:       \n",
    "                    x=encoded[1]\n",
    "                    if type(x)==list:\n",
    "                        new_window.append(x[0])\n",
    "                    else:\n",
    "                        new_window.append(x)\n",
    "                elif len(encoded)==1:\n",
    "                    if type(encoded)==list:\n",
    "                        new_window.append(encoded[0])\n",
    "                    else:\n",
    "                        new_window.append(encoded)\n",
    "                else:\n",
    "                    #for some reason it finds the unicode stuff __\n",
    "                    pass\n",
    "                    #print(window)\n",
    "                    #print(i)\n",
    "                    #print(encoded)\n",
    "                    #input()\n",
    "            #print(window)\n",
    "            #print(len(window))\n",
    "            #print(len(tokenizer.decode(window)))\n",
    "            #print(tokenizer.decode(window))\n",
    "            #print(len(tokenizer.encode(window)))\n",
    "            #window = tokenizer.encode(tokenizer.decode(window))\n",
    "            window = new_window\n",
    "        #print(window)\n",
    "        #print(len(window))\n",
    "        #input()\n",
    "\n",
    "        #save windowz\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "\n",
    "###\n",
    "# adapted from the PyTorch examples. for the full PyTorch LM example, see: \n",
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "###\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    \"\"\"Model feeds pre-trained embeddings through a series of biLSTM\n",
    "       layers, followed by a linear vocabulary decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, lstm_layers, word_vectors, \n",
    "                 dropout=0.05, bidirectional = True):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "\n",
    "        self.vocab_size = word_vectors.shape[0]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "\n",
    "        # blank embed layer starting from GloVe pre-trained vectors\n",
    "        self._embed = nn.Embedding.from_pretrained(word_vectors, freeze=False)        \n",
    "        self._drop = nn.Dropout(dropout)\n",
    "\n",
    "        self._lstm = nn.LSTM(in_dim, hidden_dim, num_layers = lstm_layers, dropout = dropout,\n",
    "                             bidirectional = bidirectional, batch_first=True)\n",
    "        self._ReLU = nn.ReLU()\n",
    "        self._pred = nn.Linear((2 if bidirectional else 1)*hidden_dim, \n",
    "                               #self.vocab_size)\n",
    "                               1) #only 1 or zeros here \n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self._drop(self._embed(x))\n",
    "        z, h = self._lstm(e)\n",
    "        z_drop = self._drop(z)\n",
    "        s = self._pred(self._ReLU(z_drop))\n",
    "        #s = s.view(-1, self.vocab_size)\n",
    "        s = s.squeeze()\n",
    "        return s, h\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.lstm_layers, batch_size, self.hidden_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "## use saved model\n",
    "#######\n",
    "\n",
    "torch.manual_seed(691)\n",
    "\n",
    "#vocab size from sentence peice\n",
    "vocab_size = 10000 #same as sentence peice\n",
    "vocab_dim = 50  # the size of our pre-trained word vectors\n",
    "\n",
    "# randomly initialize our word vectors!\n",
    "vocab_dim = 256\n",
    "word_vectors = torch.randn(vocab_size, vocab_dim)\n",
    "word_vectors.shape, word_vectors\n",
    "\n",
    "#set up model\n",
    "hidden_dim = 200\n",
    "lstm_layers = 2\n",
    "LSTM_LM_net_trained = LSTM_LM(word_vectors.shape[1], hidden_dim,lstm_layers, word_vectors)\n",
    "\n",
    "#[TODO]: fix so it works\n",
    "#https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "#https://stackoverflow.com/questions/61242966/pytorch-attributeerror-function-object-has-no-attribute-copy\n",
    "\n",
    "name = 'medium_py'\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, filepath=name+'_tokenizer.model'):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                        #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                        input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "#load weights into model\n",
    "LSTM_LM_net_trained.load_state_dict(torch.load('./data/'+name+'biLSTM_LM.pt'))\n",
    "LSTM_LM_net_trained.eval()\n",
    "\n",
    "#instantiate tokenizer model\n",
    "tokenizer = Tokenizer(name+'_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/214 [00:00<00:02, 89.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:01<00:00, 115.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 25\n",
      "\n",
      "********\n",
      "def quicksort(array):\n",
      "********\n",
      "\n",
      "    if len(array) < 2:2:\n",
      "********\n",
      "\n",
      "        return array\n",
      "********\n",
      "\n",
      "    low, same, high = [], [], []\n",
      "********\n",
      "\n",
      "    pivot = array[randint(0, len(array) - 1)]\n",
      "********\n",
      "\n",
      "    for item in array:\n",
      "********\n",
      "\n",
      "        if item < pivot:\n",
      "********\n",
      "\n",
      "            low.append(item)\n",
      "********\n",
      "\n",
      "        elif item == pivot:\n",
      "********\n",
      "\n",
      "            same.append(item)\n",
      "********\n",
      "\n",
      "        elif item > pivot:\n",
      "********\n",
      "\n",
      "            high.append(item)\n",
      "********\n",
      "\n",
      "    return quicksort(low\n",
      "********\n",
      ")\n",
      "********\n",
      " \n",
      "********\n",
      "+\n",
      "********\n",
      " \n",
      "********\n",
      "same\n",
      "********\n",
      " \n",
      "********\n",
      "+\n",
      "********\n",
      " \n",
      "********\n",
      "quick\n",
      "********\n",
      "sort\n",
      "********\n",
      "(\n",
      "********\n",
      "high\n",
      "********\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print('-----------------------------------------------------')\n",
    "    code=get_code()\n",
    "    \n",
    "    #space\n",
    "    new_code = code.replace(' ',' SPACE')\n",
    "    #newline\n",
    "    new_code = new_code.replace('\\n',' NEWLINE')\n",
    "    #tab\n",
    "    new_code = new_code.replace('\\t',' TAB')\n",
    "\n",
    "    tokens = tokenizer.encode(new_code,t=str)\n",
    "\n",
    "\n",
    "    wd=20 #window diameter\n",
    "    X_windows = centered_sliding_window(tokens,wd,encode=True)\n",
    "    #code, breaks = get_predicted_break_points(X_windows,LSTM_LM_net_trained,top=0,thresh=.5)\n",
    "    code, breaks = get_top_n_preds(X_windows,LSTM_LM_net_trained,top=25)\n",
    "    if len(breaks)>0:\n",
    "        print(i,len(breaks))\n",
    "        #'''\n",
    "        #from code segmentation file\n",
    "        comments_added = insert_comments(code,sorted(breaks))\n",
    "        comments_added_decoded = tokenizer.decode(comments_added)\n",
    "        comments_added_token_string = ''.join(comments_added_decoded)\n",
    "        comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "        comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "        comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "        print(comments_added_token_string)\n",
    "        #'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 22, 33, 53, 75, 86, 102, 121, 137, 156, 172, 191, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(breaks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
