{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "random.seed(10)\n",
    "\n",
    "from csv import reader\n",
    "from tqdm import tqdm #inline progress bar (quality of life)\n",
    "import sentencepiece as spm\n",
    "\n",
    "#SAVE DATA FOR BiLSTM\n",
    "\n",
    "import json \n",
    "\n",
    "import ast\n",
    "import tokenize\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#move data to location for analizing\n",
    "def make_data(file):\n",
    "    # open file in read mode\n",
    "    number_of_lines = 0\n",
    "    num_chars = 0 #TODO\n",
    "    with open(file, 'r') as read_obj:\n",
    "        print('loading data from ',file,'...')\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "        # Iterate over each row in the csv using reader object\n",
    "\n",
    "        data = []\n",
    "        for row in tqdm(csv_reader):\n",
    "            try:\n",
    "                # row variable is a list that represents a row in csv\n",
    "                #row[0] - code\n",
    "                #row[1] - comment\n",
    "\n",
    "                number_of_lines+=row[0].count('\\n')\n",
    "\n",
    "                #way to handle white spaces:\n",
    "                #space #do first, \n",
    "                new_code = row[0].replace(' ',' SPACE')\n",
    "                #newline\n",
    "                new_code = new_code.replace('\\n',' NEWLINE')\n",
    "                #tab\n",
    "                new_code = new_code.replace('\\t',' TAB')\n",
    "\n",
    "                #TODO\n",
    "                #FILTER FOR UNICODE THING\n",
    "\n",
    "\n",
    "                #save new data\n",
    "                row = [new_code,row[1]]\n",
    "                data.append(row)\n",
    "\n",
    "            #https://stackoverflow.com/questions/4166070/python-csv-error-line-contains-null-byte\n",
    "            #https://intellipaat.com/community/18827/how-to-delete-only-one-row-in-csv-with-python\n",
    "            except:\n",
    "                csv_reader.remove(row)\n",
    "\n",
    "\n",
    "        read_obj.close()\n",
    "\n",
    "    print('num samples: ', len(data))\n",
    "    print('num lines: ', number_of_lines)\n",
    "    \n",
    "    return data,number_of_lines\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    #def __init__(self, filepath='python_tokenizer.model'):\n",
    "    def __init__(self, filepath):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522,number_of_lines=10000):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                       #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                       input_sentence_size=number_of_lines, shuffle_input_sentence=True)\n",
    "\n",
    "#detokenize\n",
    "def decode_tokenized_code_snippet(tokens):\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    #decode but still has the added strings\n",
    "    #print(decoded)\n",
    "    token_string = ''.join(decoded)\n",
    "    token_string = token_string.replace('SPACE',' ')\n",
    "    token_string = token_string.replace('NEWLINE','\\n')\n",
    "    token_string = token_string.replace('TAB','\\t')\n",
    "    #print(token_string)\n",
    "    return token_string\n",
    "\n",
    "#print(decode_tokenized_code_snippet(tokens))\n",
    "\n",
    "\n",
    "\n",
    "def generate_code_from_data_no_edge(data,num_samples):\n",
    "    #\n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    sampled_indexes = sample(code_indixes,num_samples)\n",
    "\n",
    "    generated_code = []\n",
    "    generated_code_ids = []\n",
    "    break_locations = []\n",
    "    for i in sampled_indexes:\n",
    "        \n",
    "        #get raw code\n",
    "        #sampled_code_snippet = data[i][0]+str('\\n') #just the code, thus the 0\n",
    "        \n",
    "        #tokenize it \n",
    "        #with words\n",
    "        sampled_code_snippet = tokenizer.encode(data[i][0],t=str)\n",
    "        sampled_code_snippet_ids = tokenizer.encode(data[i][0])\n",
    "        \n",
    "        \n",
    "        generated_code+=sampled_code_snippet\n",
    "        if len(break_locations)<num_samples-1:\n",
    "            break_locations.append(len(generated_code)-1)\n",
    "        \n",
    "    return generated_code, break_locations, generated_code_ids\n",
    "\n",
    "\n",
    "def generate_valid_code_from_data_no_edge(data,num_samples):\n",
    "    \n",
    "    #valid segment search params\n",
    "    trys = 0\n",
    "    max_trys = 100\n",
    "    \n",
    "    #\n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    sampled_indexes = sample(code_indixes,num_samples)\n",
    "\n",
    "    generated_code = []\n",
    "    generated_code_ids = []\n",
    "    break_locations = []\n",
    "    \n",
    "    while len(break_locations)<num_samples:\n",
    "        \n",
    "        #get code sample from data\n",
    "        new_index = random.choice(code_indixes)\n",
    "        sampled_code_snippet = tokenizer.encode(data[new_index][0],t=str)\n",
    "        sampled_code_snippet_ids = tokenizer.encode(data[new_index][0])\n",
    "        \n",
    "        #prep for validation\n",
    "        token_string = generated_code+sampled_code_snippet\n",
    "        token_string = ''.join(token_string)\n",
    "        token_string = token_string.replace('SPACE',' ')\n",
    "        token_string = token_string.replace('NEWLINE','\\n')\n",
    "        token_string = token_string.replace('TAB','\\t')\n",
    "        \n",
    "        if check_code_validity(token_string):\n",
    "            \n",
    "            #save the relivent info\n",
    "            generated_code+=sampled_code_snippet\n",
    "            if len(break_locations)<num_samples:\n",
    "                break_locations.append(len(generated_code)-1)  \n",
    "                  \n",
    "        else:\n",
    "            #reset the test snippet\n",
    "            temp_tokenized_code=[]\n",
    "            trys+=1\n",
    "        \n",
    "    return generated_code, break_locations[:-1], generated_code_ids\n",
    "\n",
    "\n",
    "def centered_sliding_window(token_list, window_diamiter,encode=False,PAD='unk'):\n",
    "    windows = []\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        #print(token_list)\n",
    "        #input()\n",
    "        \n",
    "        window = []\n",
    "        \n",
    "        #if we have to pad the begining\n",
    "        if i < window_diamiter:\n",
    "            before_len = window_diamiter-i\n",
    "            before = [PAD]*before_len+token_list[0:i]\n",
    "        else:\n",
    "            before = token_list[i-window_diamiter:i]\n",
    "        \n",
    "        #if we have to pad the end\n",
    "        if i+window_diamiter>=len(token_list):\n",
    "            after_len = (i+1+window_diamiter)-len(token_list)\n",
    "            after = token_list[i+1:i+1+window_diamiter]+[PAD]*after_len\n",
    "\n",
    "        else:\n",
    "            after = token_list[i+1:i+1+window_diamiter]\n",
    "        \n",
    "        #put it togeather\n",
    "        #print('------')\n",
    "        #print('before:',before)\n",
    "        #print('center:',token_list[i])\n",
    "        #print('after:',after)\n",
    "        window = before + [token_list[i]] + after\n",
    "        #for encoding code if we want\n",
    "        if encode:\n",
    "            new_window = []\n",
    "            #print(window)\n",
    "            #input()\n",
    "            for i in window:\n",
    "                encoded = tokenizer.encode(i)\n",
    "                if len(encoded)>1:       \n",
    "                    x=encoded[1]\n",
    "                    if type(x)==list:\n",
    "                        new_window.append(x[0])\n",
    "                    else:\n",
    "                        new_window.append(x)\n",
    "                elif len(encoded)==1:\n",
    "                    if type(encoded)==list:\n",
    "                        new_window.append(encoded[0])\n",
    "                    else:\n",
    "                        new_window.append(encoded)\n",
    "                else:\n",
    "                    #for some reason it finds the unicode stuff __\n",
    "                    pass\n",
    "                    #print(window)\n",
    "                    #print(i)\n",
    "                    #print(encoded)\n",
    "                    #input()\n",
    "            #print(window)\n",
    "            #print(len(window))\n",
    "            #print(len(tokenizer.decode(window)))\n",
    "            #print(tokenizer.decode(window))\n",
    "            #print(len(tokenizer.encode(window)))\n",
    "            #window = tokenizer.encode(tokenizer.decode(window))\n",
    "            window = new_window\n",
    "        #print(window)\n",
    "        #print(len(window))\n",
    "        #input()\n",
    "\n",
    "        #save windowz\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "#put comment in at break points\n",
    "#https://www.tutorialspoint.com/python/list_insert.htm\n",
    "#list.insert(index, obj)\n",
    "def insert_comments(code, break_spots, comment='\\n'+'*'*8+'\\n',at_begining=True):\n",
    "    #if there is a a comment at begining of snippet\n",
    "    #if at_begining:\n",
    "        #adds a notation to add a 0\n",
    "        #at beigning of break spots too\n",
    "    #    break_spots.insert(0,0)\n",
    "    \n",
    "    \n",
    "    #go through breaks backwards\n",
    "    #so as not to mess up break \n",
    "    #spots as we would if we went forward\n",
    "    for b in break_spots[::-1]:\n",
    "        code.insert(b,comment)\n",
    "    return code\n",
    "\n",
    "\n",
    "#code=['a','b','c','d']\n",
    "#c = insert_comments(code,[2])\n",
    "#print(c)\n",
    "\n",
    "def make_data_points(num_segments_per_MSC = 5, window_diameter=20):\n",
    "    \n",
    "    \n",
    "    #code, breaks, _ = generate_code_from_data(data,3) #WORKS\n",
    "    #code, breaks, _ =generate_code_from_data_mark_begiging(data,num_segments_per_MSC)\n",
    "    #code, breaks, _ = generate_code_from_data_no_edge(data,num_segments_per_MSC)\n",
    "    code, breaks, _ = generate_valid_code_from_data_no_edge(data,num_segments_per_MSC)\n",
    "    #code, breaks, _ = generate_code_from_data_mark_begiging(data,3)\n",
    "    #code, breaks = generate_aligned_code_from_data_2(data,3) #left in old dir, in code segmentation files (should be 2 of them)\n",
    "    #code, breaks = generate_verified_code_from_data(data,3)  #left in old dir, in code segmentation files (should be 2 of them)\n",
    "    \n",
    "    #do it with the code tokens\n",
    "    wd= window_diameter\n",
    "    X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "    \n",
    "    #do it with the ground truth \n",
    "    y = [0]*len(code)\n",
    "    for b in breaks:\n",
    "        y[b] = 1\n",
    "    Y_windows = centered_sliding_window(y,wd,encode=False,PAD=0)\n",
    "    #print(Y_windows)\n",
    "    \n",
    "    return X_windows, Y_windows\n",
    "\n",
    "def make_generated_code_dataset(num_snippets, name):\n",
    "    # Data to be written \n",
    "    LSMT_DATA = {}\n",
    "    for i in tqdm(range(num_snippets)):\n",
    "        x,y = make_data_points()\n",
    "        LSMT_DATA[str(i)] = {'x':x,'y':y}\n",
    "\n",
    "    with open(name+\"_LSTM_DATA.json\", \"w\") as outfile: \n",
    "        json.dump(LSMT_DATA, outfile)\n",
    "        \n",
    "def check_code_validity(code_snippet):\n",
    "    valid=False\n",
    "    try:\n",
    "        code_snippet = decode_tokenized_code_snippet(code_snippet)\n",
    "        tokens =  tokenize.tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\n",
    "        for t in tokens:\n",
    "            #this is enough to trigger it\n",
    "            pass\n",
    "        valid=True\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(e)\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47256it [00:00, 227909.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from  code-comment-short_py.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384274it [00:01, 223973.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  384274\n",
      "num lines:  1379956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "data, number_of_lines = make_data('code-comment-short_py.csv')\n",
    "#instantiate tokenizer model\n",
    "tokenizer = Tokenizer('short_py_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11, 3, 5, 16, 5, 3909, 7, 6, 10, 3, 50, 9, 4, 3, 3, 3, 3, 4, 3, 3, 3, 12, 3, 6, 15, 34, 8, 16, 7, 50, 10, 3, 27, 13, 4, 4]\n",
      "--------------------------------\n",
      "code:\n",
      "--------------------------------\n",
      "TEMPLATE_FILE  =  \"testcircuits/index_template.html\" \n",
      "        def  accept_comment(self,  comment_id): \n",
      "                 \n",
      "                raise  NotImplementedError() \n",
      "        def  connected(self): \n",
      "                 \n",
      "                return  self._connected \n",
      " \n",
      " class  FlowBuilderTransportError(XDDError): \n",
      "         \n",
      " \n",
      "class  FlowBuilderTransportForwardingServer(SocketServer.TCPServer): \n",
      "        def  pack(self,  obj): \n",
      "                 \n",
      "                return  pickle.dumps(self._box(obj),  protocol  =  PICKLE_PROTOCOL) \n",
      " \n",
      "\n",
      "--------------------------------\n",
      "break spots:  [18, 56, 89, 123]\n",
      "--------------------------------\n",
      "code with break spots indicated:\n",
      "--------------------------------\n",
      "TEMPLATE_FILE  =  \"testcircuits/index_template.html\"\n",
      "********\n",
      " \n",
      "        def  accept_comment(self,  comment_id): \n",
      "                 \n",
      "                raise  NotImplementedError()\n",
      "********\n",
      " \n",
      "        def  connected(self): \n",
      "                 \n",
      "                return  self._connected \n",
      "\n",
      "********\n",
      " \n",
      " class  FlowBuilderTransportError(XDDError): \n",
      "         \n",
      " \n",
      "class  FlowBuilderTransportForwardingServer(SocketServer.TCPServer):\n",
      "********\n",
      " \n",
      "        def  pack(self,  obj): \n",
      "                 \n",
      "                return  pickle.dumps(self._box(obj),  protocol  =  PICKLE_PROTOCOL) \n",
      " \n",
      "\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "#EAMPLES and TESTS\n",
    "\n",
    "\n",
    "#tokenize code (ie encode)\n",
    "#with words\n",
    "#tokens = tokenizer.encode(data[1][0],t=str)\n",
    "#with numbers\n",
    "tokens = tokenizer.encode(data[1][0])\n",
    "print(tokens)\n",
    "\n",
    "'''\n",
    "num_segments_per_MSC = 10\n",
    "code, breaks, _ = generate_code_from_data_mark_begiging(data,num_segments_per_MSC)\n",
    "#do it with the code tokens\n",
    "wd=20 #window diameter\n",
    "X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "#print(X_windows)\n",
    "#print(len(X_windows))\n",
    "'''\n",
    "\n",
    "#example\n",
    "'''\n",
    "wd=3#window diameter\n",
    "l = ['a','b','c','d','e']#,'f','g','h','i','j','k','l']\n",
    "windows = centered_sliding_window(l,wd,encode=False)\n",
    "for w in windows:\n",
    "    #print(len(w))\n",
    "    print('center:',w[int(len(w)/2)],'- window: ',w)\n",
    "'''\n",
    "\n",
    "#simplest tokenizer example\n",
    "#tokenizer.decode(201)\n",
    "\n",
    "####\n",
    "####ALIGNMENT ISSUES, UNCOMMENT THE 'good' AND 'bad' PRINT,\n",
    "####SEE SOEMTIMES WHEN UNICODE CHAR IS THERE IT MESSES UP\n",
    "####\n",
    "\n",
    "num_segments_per_MSC = 5\n",
    "code, breaks, _ = generate_valid_code_from_data_no_edge(data,num_segments_per_MSC)\n",
    "#code, breaks, _ = generate_code_from_data_no_edge(data,num_segments_per_MSC)\n",
    "#code, breaks, _ = generate_code_from_data_mark_begiging(data,num_segments_per_MSC)\n",
    "#code, breaks, _ = generate_code_from_data(data,3)\n",
    "#code, breaks = generate_aligned_code_from_data_2(data,3)\n",
    "#code, breaks = generate_verified_code_from_data(data,3)\n",
    "print('--------------------------------')\n",
    "print('code:')\n",
    "print('--------------------------------')\n",
    "decoded = tokenizer.decode(code)\n",
    "token_string = ''.join(decoded)\n",
    "token_string = token_string.replace('SPACE',' ')\n",
    "token_string = token_string.replace('NEWLINE','\\n')\n",
    "token_string = token_string.replace('TAB','\\t')\n",
    "print(token_string)\n",
    "#print(code)\n",
    "'''\n",
    "for i in range(len(code)):\n",
    "    if i in breaks:\n",
    "        print(code[i],'------')\n",
    "    else:\n",
    "        print(code[i])\n",
    "'''\n",
    "print('--------------------------------')\n",
    "print('break spots: ',breaks)\n",
    "print('--------------------------------')\n",
    "print('code with break spots indicated:')\n",
    "print('--------------------------------')\n",
    "#insert_comments\n",
    "comments_added = insert_comments(code,breaks)\n",
    "comments_added_decoded = tokenizer.decode(comments_added)\n",
    "comments_added_token_string = ''.join(comments_added_decoded)\n",
    "comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "print(comments_added_token_string)\n",
    "print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11101it [00:00, 68105.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trianing... py short\n",
      "loading data from  code-comment-short_py.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384274it [00:01, 230968.34it/s]\n",
      "  0%|          | 3/10000 [00:00<06:41, 24.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  384274\n",
      "num lines:  1379956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:32<00:00, 25.45it/s]\n",
      "17808it [00:00, 178079.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trianing... py medium\n",
      "loading data from  code-comment-medium_py.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1090472it [00:06, 175684.14it/s]\n",
      "  0%|          | 2/10000 [00:00<12:06, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  1090472\n",
      "num lines:  6496023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [12:48<00:00, 13.02it/s]\n",
      "18955it [00:00, 93047.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trianing... py long\n",
      "loading data from  code-comment-long_py.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355397it [00:03, 95311.11it/s]\n",
      "  0%|          | 1/10000 [00:00<24:09,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  355397\n",
      "num lines:  4717632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [25:50<00:00,  6.45it/s] \n"
     ]
    }
   ],
   "source": [
    "#lang='py'\n",
    "#lang='cpp'\n",
    "#lang='java'\n",
    "#lang='all'\n",
    "\n",
    "for i in ['py']:#,'cpp','java']:\n",
    "    for j in ['short','medium','long']:\n",
    "        print('trianing...',i,j)\n",
    "        name = j+'_'+i\n",
    "        file = 'code-comment-'+name+'.csv'\n",
    "        #get data\n",
    "        data, number_of_lines = make_data(file)\n",
    "        #instantiate tokenizer model\n",
    "        tokenizer = Tokenizer(name+'_tokenizer.model')\n",
    "        #gereate the dataset for the lstm\n",
    "        make_generated_code_dataset(10000, name)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
